{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def file_to_data(path):\n",
    "#     try:\n",
    "#         with open(path) as f:\n",
    "#             data = [[x.rstrip().split('\\t')[1], x.rstrip().split('\\t')[2], x.rstrip().split('\\t')[0]]  for x in f.readlines()]\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"File does not exist\")\n",
    "#         return\n",
    "    \n",
    "#     formatted_data = []\n",
    "#     for row in data:\n",
    "#         formatted_data.append([row[0], row[1], row[2]])\n",
    "\n",
    "#     return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = '2016'\n",
    "# data_file = 'postediting.test.tsv'\n",
    "# path = f'data/sts/semeval-sts/{year}/{data_file}'\n",
    "# data = file_to_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two young, White males are outside near many bushes.',\n",
       " 'Several men in hard hats are operating a giant pulley system.',\n",
       " 'A little girl climbing into a wooden playhouse.',\n",
       " 'A man in a blue shirt is standing on a ladder cleaning a window.',\n",
       " 'Two men are at the stove preparing food.']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/Multi-30k/train.en') as f:\n",
    "    data = [x.strip() for x in f.readlines()]\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dat in data[15000:20000]:\n",
    "#     text_data.append(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60265"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_data = list(set(text_data))\n",
    "# len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/base_text_data.pkl', 'rb') as f:\n",
    "    text_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.punctuations = [r'\\.', r'\\.{2,}',\n",
    "                             r'\\!+', r'\\:+', r'\\;+', r'\\\"+', r\"\\'+\", r'\\?+', r'\\,+', r'\\(|\\)|\\[|\\]|\\{|\\}|\\<|\\>']\n",
    "\n",
    "    def clean(self, line):\n",
    "        for pattern in self.punctuations:\n",
    "            line = re.sub(pattern, '', line)\n",
    "        line = re.sub(r'[^a-z]', ' ', line.lower())\n",
    "        return line\n",
    "\n",
    "    def tokenize(self, line):\n",
    "        line = self.clean(line)\n",
    "        return line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review:\n",
    "    tokenizer = Tokenizer()\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.tokens = Review.tokenizer.tokenize(self.text)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tokens)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "for r in text_data:\n",
    "    training_data.append(Review(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        input_embeds = self.embeddings(inputs)\n",
    "        embeds = torch.mean(input_embeds, dim=1)\n",
    "        out = self.linear(embeds)\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, reviews, context_size=2, embedding_size=50, oov_threshold=2, neg_sample_size=5, lr=0.001):\n",
    "\n",
    "        self.reviews = reviews\n",
    "        self.oov_threshold = oov_threshold\n",
    "        self.oov_token = '<OOV>'\n",
    "        self.context_size = context_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.vocabulary = {self.oov_token}\n",
    "        self.vocab_idx = {self.oov_token: 0}\n",
    "        self.vocab_ridx = {0: self.oov_token}\n",
    "\n",
    "        self.freq = defaultdict(int)\n",
    "        self.freq_dist = [0]\n",
    "        self.total_word_count = 0\n",
    "\n",
    "        self.build_vocabulary()\n",
    "\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.neg_sample_size = neg_sample_size\n",
    "\n",
    "        self.model = CBOW(self.N, self.embedding_size)\n",
    "        self.dataset = self.build_dataset()\n",
    "        self.weights = self.negative_sampling()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr)\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "        \n",
    "        print(\"Building Vocabulary\")\n",
    "        for review in tqdm(self.reviews):\n",
    "            for token in review:\n",
    "                self.freq[token] += 1\n",
    "\n",
    "        index = 1\n",
    "        for token, f in self.freq.items():\n",
    "            if f > self.oov_threshold:\n",
    "                self.vocabulary.add(token)\n",
    "                self.vocab_idx[token] = index\n",
    "                self.vocab_ridx[index] = token\n",
    "                self.freq_dist.append(f)\n",
    "                index += 1\n",
    "            else:\n",
    "                self.freq_dist[0] += f\n",
    "\n",
    "        self.total_word_count = sum(self.freq.values())        \n",
    "        self.N = len(self.vocabulary)\n",
    "        print(f\"Total Vocabulary Size: {self.N}\")\n",
    "\n",
    "    def build_dataset(self):\n",
    "\n",
    "        print(\"Building Dataset\")\n",
    "        dataset = []\n",
    "        for review in tqdm(self.reviews):\n",
    "            for i in range(self.context_size, len(review) - self.context_size):\n",
    "                focus = review[i]\n",
    "                if focus not in self.vocabulary:\n",
    "                    focus = self.oov_token\n",
    "                focus_index = self.vocab_idx[focus]\n",
    "                context_indices = []\n",
    "                for j in range(i - self.context_size, i + self.context_size + 1):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    context = review[j]\n",
    "                    if context not in self.vocabulary:\n",
    "                        context = self.oov_token\n",
    "                    context_index = self.vocab_idx[context]\n",
    "                    context_indices.append(context_index)\n",
    "                dataset.append((context_indices, focus_index))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def negative_sampling(self):\n",
    "        print(\"Computing Weights\")\n",
    "        normalized_freq = F.normalize(torch.Tensor(self.freq_dist).pow(0.75), dim=0)\n",
    "        weights = torch.ones(len(self.freq_dist))\n",
    "\n",
    "        for _ in tqdm(range(len(self.freq_dist))):\n",
    "            for _ in range(self.neg_sample_size):\n",
    "                neg_index = torch.multinomial(normalized_freq, 1)[0]\n",
    "                weights[neg_index] += 1\n",
    "        \n",
    "        return weights\n",
    "\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        losses = []\n",
    "        loss_fn = nn.NLLLoss(weight=self.weights)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            net_loss = 0\n",
    "            for i in tqdm(range(0, len(self.dataset), self.BATCH_SIZE)):\n",
    "                batch = self.dataset[i: i+self.BATCH_SIZE]\n",
    "\n",
    "                context = [x[0] for x in batch]\n",
    "                focus = [x[1] for x in batch]\n",
    "\n",
    "                context_var = Variable(torch.LongTensor(context))\n",
    "                focus_var = Variable(torch.LongTensor(focus))\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                log_probs = self.model(context_var)\n",
    "                loss = loss_fn(log_probs, focus_var)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                net_loss += loss.item()\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            losses.append(net_loss)\n",
    "\n",
    "    \n",
    "    def get_embedding(self, word_idx):\n",
    "        embedding_index = Variable(torch.LongTensor([word_idx]))\n",
    "        return self.model.embeddings(embedding_index).data[0]\n",
    "    \n",
    "    def get_closest_vector(self, _word, k):\n",
    "        \n",
    "        word = _word.lower()\n",
    "\n",
    "        if word not in self.vocabulary:\n",
    "            word = self.oov_token\n",
    "\n",
    "        distances = []\n",
    "        focus_index = self.vocab_idx[word]\n",
    "        focus_embedding = self.get_embedding(focus_index)\n",
    "\n",
    "        for i in range(1, self.N):\n",
    "            if i == focus_index:\n",
    "                continue\n",
    "        \n",
    "            comp_embedding = self.get_embedding(i)\n",
    "            comp_word = self.vocab_ridx[i]\n",
    "            dist = cosine(focus_embedding, comp_embedding)\n",
    "            distances.append({'Word': comp_word, 'Distance': dist})\n",
    "        \n",
    "        distances = sorted(distances, key=lambda x: x['Distance'])\n",
    "\n",
    "        return [x['Word'] for x in distances[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60265/60265 [00:00<00:00, 394942.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 12907\n",
      "Building Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60265/60265 [00:01<00:00, 39110.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12907/12907 [00:28<00:00, 460.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7848/7848 [02:49<00:00, 46.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.6111623048782349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = Word2Vec(training_data)\n",
    "encoder.train(1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c8b4332f17b55abe5b67c7fb84c98e7a77ec1afd164452ea9e618fbfab6a3af"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
