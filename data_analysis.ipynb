{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.regex_subs = {\n",
    "            r'(https?:\\/\\/)\\S+': \"0URL0\",\n",
    "            r'(?<!http://)www\\.\\S+': \"0URL0\",\n",
    "            r'(\\W)(?=\\1)': '',\n",
    "            r'(?<=[a-zA-Z])(\\-)(?=[a-zA-Z])': ''\n",
    "        }\n",
    "\n",
    "        self.punctuations = [r'\\.', r'\\.{2,}',\n",
    "                             r'\\!+', r'\\:+', r'\\;+', r'\\\"+', r\"\\'+\", r'\\?+', r'\\,+', r'\\(|\\)|\\[|\\]|\\{|\\}|\\<|\\>']\n",
    "\n",
    "        self.delimiter = '<SPLIT>'\n",
    "        self.stemmer = SnowballStemmer(language='english')    \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def clean_line(self, line):\n",
    "        for pattern, rep in self.regex_subs.items():\n",
    "            line = re.sub(pattern, rep, line)\n",
    "        for pattern in self.punctuations:\n",
    "            line = re.sub(pattern, '', line)\n",
    "        return line.lower()\n",
    "\n",
    "    def tokenize_line(self, line):\n",
    "        line = re.sub('\\s+', self.delimiter, line)\n",
    "\n",
    "        token_list = [x.strip()\n",
    "                      for x in line.split(self.delimiter) if x.strip() != '']\n",
    "\n",
    "        return token_list\n",
    "\n",
    "    def clean_and_tokenize(self, lines):\n",
    "        if isinstance(lines, list):\n",
    "            cleaned_tokens = []\n",
    "            for line in lines:\n",
    "                if not len(line.strip()):\n",
    "                    continue\n",
    "                line = self.clean_line(line)\n",
    "                tokens = self.tokenize_line(line)\n",
    "                cleaned_tokens.append(tokens)\n",
    "            return cleaned_tokens\n",
    "        else:\n",
    "            line = self.clean_line(lines)\n",
    "            tokens = self.tokenize_line(line)\n",
    "            return tokens\n",
    "\n",
    "    def _clean(self, line):\n",
    "        line = self.clean_line(line)\n",
    "        \n",
    "        cleaned = []\n",
    "        for token in line.split():\n",
    "            if token not in self.stop_words:\n",
    "                cleaned.append(self.stemmer.stem(token))\n",
    "\n",
    "        return \" \".join(cleaned)\n",
    "\n",
    "    def clean(self, lines):\n",
    "        if isinstance(lines, list):\n",
    "            cleaned_lines = []\n",
    "            for line in lines:\n",
    "                if not len(line.strip()):\n",
    "                    continue\n",
    "                line = self._clean(line)\n",
    "                cleaned_lines.append(line)\n",
    "            return cleaned_lines\n",
    "        else:\n",
    "            line = self._clean(lines)\n",
    "            return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_data(path):\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            data = [[x.rstrip().split('\\t')[1], x.rstrip().split('\\t')[2], x.rstrip().split('\\t')[3]]  for x in f.readlines()]\n",
    "    except FileNotFoundError:\n",
    "        print(\"File does not exist\")\n",
    "        return\n",
    "    \n",
    "    formatted_data = []\n",
    "    for row in data[1:]:\n",
    "        formatted_data.append([row[0], row[1], row[2]])\n",
    "\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A group of kids is playing in a yard and an old man is standing in the background', 'A group of boys in a yard is playing and a man is standing in the background', '4.5'] 4500\n"
     ]
    }
   ],
   "source": [
    "year = '2014'\n",
    "data_file = 'images.test.tsv'\n",
    "path = f'data/sts/semeval-sts/{year}/{data_file}'\n",
    "data = file_to_data(\"data/sts/sick2014/SICK_train.txt\")\n",
    "print(data[0], len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF_clean(data):\n",
    "    tokenizer = Tokenizer()\n",
    "    formatted_data = []\n",
    "    for row in data:\n",
    "        formatted_data.append([tokenizer.clean(row[0]), tokenizer.clean(row[1]), row[2]])\n",
    "    \n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_data = TFIDF_clean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_test_split(data, test_size=0.15):\n",
    "    X, y = [], []\n",
    "\n",
    "    for row in data:\n",
    "        X.append([row[0], row[1]])\n",
    "        y.append(float(row[2]))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=69)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3825 675\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_dev_test_split(tok_data)\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('data/train_data.pkl', 'wb') as f:\n",
    "#     pickle.dump({'x': X_train, 'y': y_train}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizer():\n",
    "    def __init__(self):\n",
    "        self.tf_l = []\n",
    "        self.tf_r = []\n",
    "\n",
    "        self.idf = defaultdict(int)\n",
    "\n",
    "        self.vocab = {}\n",
    "        self.data_l = None\n",
    "        self.data_r = None\n",
    "        self.vocab_len = 0\n",
    "        self.num_docs = 0\n",
    "\n",
    "    def create_vocab(self, data):\n",
    "        print(\"Creating Vocabulary...\")\n",
    "        self.data_l = []\n",
    "        self.data_r = []\n",
    "        for items in data:\n",
    "            self.data_l.append(items[0].split())\n",
    "            self.data_r.append(items[1].split())\n",
    "        self.num_docs = len(self.data_l)\n",
    "\n",
    "        for text in self.data_l:\n",
    "            for token in text:\n",
    "                if not token in self.vocab:\n",
    "                    self.vocab[token] = self.vocab_len\n",
    "                    self.vocab_len += 1\n",
    "        \n",
    "        for text in self.data_r:\n",
    "            for token in text:\n",
    "                if not token in self.vocab:\n",
    "                    self.vocab[token] = self.vocab_len\n",
    "                    self.vocab_len += 1\n",
    "\n",
    "    def compute_tf(self):\n",
    "        print(\"Computing TF Scores...\")\n",
    "        for text in self.data_l:\n",
    "            d = defaultdict(int)\n",
    "            for token in text:\n",
    "                d[self.vocab[token]] += 1\n",
    "            self.tf_l.append(d)\n",
    "        \n",
    "        for text in self.data_r:\n",
    "            d = defaultdict(int)\n",
    "            for token in text:\n",
    "                d[self.vocab[token]] += 1\n",
    "            self.tf_r.append(d)\n",
    "    \n",
    "    def compute_idf(self):\n",
    "        print(\"Computing IDF Scores...\")\n",
    "        for token in self.vocab:\n",
    "            df = 0\n",
    "            for text in self.data_l:\n",
    "                if token in text:\n",
    "                    df += 1\n",
    "            \n",
    "            for text in self.data_r:\n",
    "                if token in text:\n",
    "                    df += 1\n",
    "        \n",
    "            self.idf[self.vocab[token]] = math.log((1 + self.num_docs)/(1 + df)) + 1\n",
    "     \n",
    "    def fit_transform(self, data):\n",
    "        self.create_vocab(data)\n",
    "        self.compute_tf()\n",
    "        self.compute_idf()\n",
    "        print(\"Creating TF-IDF Vectors...\")\n",
    "        X_l = np.zeros((self.num_docs, self.vocab_len), dtype='float32')\n",
    "        X_r = np.zeros((self.num_docs, self.vocab_len), dtype='float32')\n",
    "\n",
    "        for i in range(self.num_docs):\n",
    "            for token in self.data_l[i]:\n",
    "                X_l[i][self.vocab[token]] = self.tf_l[i][self.vocab[token]] * self.idf[self.vocab[token]]\n",
    "        \n",
    "        for i in range(self.num_docs):\n",
    "            for token in self.data_r[i]:\n",
    "                X_r[i][self.vocab[token]] = self.tf_r[i][self.vocab[token]] * self.idf[self.vocab[token]]\n",
    "        \n",
    "        return X_l, X_r\n",
    "    \n",
    "    def transform(self, data):\n",
    "        data_l = []\n",
    "        data_r = []\n",
    "        tf_l = []\n",
    "        tf_r = []\n",
    "        num_docs = len(data)\n",
    "\n",
    "        for items in data:\n",
    "            data_l.append(items[0].split())\n",
    "            data_r.append(items[1].split())\n",
    "\n",
    "        for text in data_l:\n",
    "            d = defaultdict(int)\n",
    "            for token in text:\n",
    "                if token in self.vocab:\n",
    "                    d[self.vocab[token]] += 1\n",
    "            tf_l.append(d)\n",
    "\n",
    "        for text in data_r:\n",
    "            d = defaultdict(int)\n",
    "            for token in text:\n",
    "                if token in self.vocab:\n",
    "                    d[self.vocab[token]] += 1\n",
    "            tf_r.append(d)\n",
    "        \n",
    "        X_l = np.zeros((num_docs, self.vocab_len), dtype='float32')\n",
    "        X_r = np.zeros((num_docs, self.vocab_len), dtype='float32')\n",
    "\n",
    "        for i in range(num_docs):\n",
    "            for token in data_l[i]:\n",
    "                if token in self.vocab:\n",
    "                    X_l[i][self.vocab[token]] = tf_l[i][self.vocab[token]] * self.idf[self.vocab[token]]\n",
    "        \n",
    "        for i in range(num_docs):\n",
    "            for token in data_r[i]:\n",
    "                if token in self.vocab:\n",
    "                    X_r[i][self.vocab[token]] = tf_r[i][self.vocab[token]] * self.idf[self.vocab[token]]\n",
    "        \n",
    "        return X_l, X_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vocabulary...\n",
      "Computing TF Scores...\n",
      "Computing IDF Scores...\n",
      "Creating TF-IDF Vectors...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3825, 1590)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_l, X_train_r = vectorizer.fit_transform(X_train)\n",
    "X_train_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "#     pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_1, vec_2):\n",
    "    return vec_1@vec_2.T/(np.linalg.norm(vec_1) * np.linalg.norm(vec_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675, 1590)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_l, X_dev_r = vectorizer.transform(X_test)\n",
    "X_dev_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_preds(data_l, data_r):\n",
    "    return [5 * (0.1 if cosine_similarity(data_l[i], data_r[i]) == 0.0 else cosine_similarity(data_l[i], data_r[i])) for i in range(len(data_l))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6605509998061392"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = output_preds(X_dev_l, X_dev_r)\n",
    "pearson_score, _ = scipy.stats.pearsonr(preds, y_test)\n",
    "pearson_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Steps\n",
    "- Get TFIDF Vectors\n",
    "- Compare vectors (cosine similarity) and scale between (0, 5) [Pipeline] (**Unsupervised/Semi-Supervised**)\n",
    "  - Show its disadvantages of being unsupervised:\n",
    "    - Lack of reliable vocabulary (highly dependent on training vocab set and common words in test set)\n",
    "    - Context (Esssential for meaning) is not captured in TF-IDF\n",
    "- Attempt TFIDF for multilingual (en-es), create TF-IDF vectors for different languages\n",
    "  - Concat the two vectors (can't be compared straight-forwardly by just taking cosine similarity, *since different word token bases, no alignment*)\n",
    "  - Pass through a linear NN to predict the training similarities (**Supervised**)\n",
    "  - Can act as a baseline for multilingual STS. Through this method we don't have to worry about alignment of the tokens of TF-IDF for different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2476b43a1381252f3e4867362f308f1b5866abd3c80db7d3bc84a9d7006e407b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
