{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from src.Vocab import Vocab\n",
    "from src.utils import cosine\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/en_en/train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/en_en/test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/english/base_text_data.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(sentences, remove_stopwords=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = [vocab.sequencify(sen, addEOSBOS=True) for sen in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sorted(sequences, key=lambda x: len(x))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx][:-1], self.sequences[idx][1:]\n",
    "\n",
    "\n",
    "def pad_batch(batch_data):\n",
    "    X = [list(x[0]) for x in batch_data]\n",
    "    y = [list(x[1]) for x in batch_data]\n",
    "\n",
    "    x_seq_len = max(len(x) for x in X)\n",
    "    y_seq_len = max(len(x) for x in y)\n",
    "\n",
    "    padded_X = np.zeros((len(X), x_seq_len))\n",
    "    padded_y = np.zeros((len(y), y_seq_len))\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        curr_X = X[i] + ([0] * (x_seq_len - len(X[i])))\n",
    "        curr_y = y[i] + ([0] * (y_seq_len - len(y[i])))\n",
    "\n",
    "        padded_X[i] = np.array(curr_X)\n",
    "        padded_y[i] = np.array(curr_y)\n",
    "\n",
    "    return torch.tensor(padded_X).long(), torch.tensor(padded_y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dataset = EmbeddingDataset(train_sequences)\n",
    "embedding_dataloader = DataLoader(embedding_dataset, batch_size=BATCH_SIZE, drop_last=True, collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers, embedding_size=128, n_hidden=256, drop_prob=0.2):\n",
    "        super(LSTMLM, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.vocab_size, self.embedding_size, padding_idx=0)\n",
    "\n",
    "        self.embedding.load_state_dict(torch.load('glove_state_dict.pkl'))\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.n_hidden, self.n_layers,\n",
    "                            dropout=self.drop_prob, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        self.fc = nn.Linear(2 * self.n_hidden, self.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x)\n",
    "        out, h = self.lstm(embedding)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMLM(\n",
       "  (embedding): Embedding(12935, 100, padding_idx=0)\n",
       "  (lstm): LSTM(100, 256, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=12935, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = LSTMLM(len(vocab), 4, EMBEDDING_DIM, 256)\n",
    "lstm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:LSTMLM, dataloader:DataLoader, device:torch.device, n_epochs:int=5)->None:\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for param in model.embedding.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        loss_val = 0\n",
    "\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, _ = model(X)\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_val += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {e+1}, Train Loss: {loss_val/len(dataloader)}\")\n",
    "\n",
    "    for param in model.embedding.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.623719723394396\n",
      "Epoch 2, Train Loss: 1.3306020590754801\n",
      "Epoch 3, Train Loss: 0.6591009895429094\n",
      "Epoch 4, Train Loss: 0.3663934275555433\n",
      "Epoch 5, Train Loss: 0.22253424891218243\n",
      "Epoch 6, Train Loss: 0.14605610603892208\n",
      "Epoch 7, Train Loss: 0.10329464575182745\n",
      "Epoch 8, Train Loss: 0.07565875822335767\n",
      "Epoch 9, Train Loss: 0.060571553530003264\n",
      "Epoch 10, Train Loss: 0.048718316512188296\n",
      "Epoch 11, Train Loss: 0.040799829863431475\n",
      "Epoch 12, Train Loss: 0.036183806848845874\n",
      "Epoch 13, Train Loss: 0.030608395506468513\n",
      "Epoch 14, Train Loss: 0.027711896432660654\n",
      "Epoch 15, Train Loss: 0.025921025639053484\n"
     ]
    }
   ],
   "source": [
    "train(lstm, embedding_dataloader, device, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_embedding(model: LSTMLM, device:torch.device, seq:np.ndarray):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X = torch.tensor(seq).to(device).unsqueeze(0)\n",
    "        _, (h, c) = model(X)\n",
    "\n",
    "    return ((torch.mean(h, dim=0).squeeze(0) + torch.mean(c, dim=0).squeeze(0))).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(model:LSTMLM, device:torch.device, seq:np.ndarray):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X = torch.tensor(seq).to(device)\n",
    "        emb = torch.mean(model.embedding(X), dim=0).detach().cpu().numpy()\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_preds = []\n",
    "\n",
    "for pair in test_data['x']:\n",
    "    s1, s2 = vocab.sequencify(pair[0]), vocab.sequencify(pair[1])\n",
    "\n",
    "    emb1 = get_context_embedding(lstm, device, s1)\n",
    "    emb2 = get_context_embedding(lstm, device, s2)\n",
    "\n",
    "    score = cosine(emb1, emb2) * 5\n",
    "    context_preds.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_preds = []\n",
    "\n",
    "for pair in test_data['x']:\n",
    "    s1, s2 = vocab.sequencify(pair[0]), vocab.sequencify(pair[1])\n",
    "\n",
    "    emb1 = get_sentence_embedding(lstm, device, s1)\n",
    "    emb2 = get_sentence_embedding(lstm, device, s2)\n",
    "\n",
    "    score = cosine(emb1, emb2) * 5\n",
    "    sentence_preds.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Score for LSTM Model (from hidden state) with Cosine Similarity: 0.4450\n"
     ]
    }
   ],
   "source": [
    "pearson_score, _ = pearsonr(context_preds, test_data['y'])\n",
    "print(f'Pearson Score for LSTM Model (from hidden state) with Cosine Similarity: {pearson_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Score for LSTM Model (from embedding layer ) with Cosine Similarity: 0.6475\n"
     ]
    }
   ],
   "source": [
    "pearson_score, _ = pearsonr(sentence_preds, test_data['y'])\n",
    "print(f'Pearson Score for LSTM Model (from embedding layer ) with Cosine Similarity: {pearson_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_ft_lstm = deepcopy(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSDataset(Dataset):\n",
    "    def __init__(self, vocab:Vocab, data):\n",
    "        self.sts_data = []\n",
    "\n",
    "        for x, y in zip(data['x'], data['y']):\n",
    "            s1, s2 = x\n",
    "            s1, s2 = vocab.sequencify(s1, addEOSBOS=True), vocab.sequencify(s2, addEOSBOS=True)\n",
    "            self.sts_data.append(((s1, s2), y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sts_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sts_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dataset = STSDataset(vocab, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dataloader = DataLoader(ft_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: zip(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_ft(model:LSTMLM, device:torch.device, dataloader:DataLoader, n_epochs:int=5):\n",
    "    criterion = nn.CosineSimilarity(dim = 0)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    model.train()\n",
    "    for e in range(n_epochs):\n",
    "        loss_val = 0\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            s1, s2  = X[0]\n",
    "            s1 = torch.tensor(s1).long().to(device).unsqueeze(0)\n",
    "            s2 = torch.tensor(s2).long().to(device).unsqueeze(0)\n",
    "\n",
    "            y = torch.tensor(y).float().to(device).squeeze(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            _, (h1, c1) = model(s1)\n",
    "            \n",
    "            emb1 = (torch.mean(h1, dim=0).squeeze(0) + torch.mean(c1, dim=0).squeeze(0))/2\n",
    "\n",
    "            _, (h2, c2) = model(s2)\n",
    "\n",
    "            emb2 = (torch.mean(h2, dim=0).squeeze(0) +torch.mean(c2, dim=0).squeeze(0)) / 2\n",
    "\n",
    "            sim = criterion(emb1, emb2) * 5\n",
    "            \n",
    "            loss = nn.MSELoss()(sim, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_val += loss.item()\n",
    "\n",
    "        print(f\"Epoch {e+1}, Train Loss: {loss_val/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/dhruv.kapur/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/rnn.py:761: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755849709/work/aten/src/ATen/native/cudnn/RNN.cpp:926.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.9654646384515881\n",
      "Epoch 2, Train Loss: 0.5156356550053578\n",
      "Epoch 3, Train Loss: 0.37323116740672124\n",
      "Epoch 4, Train Loss: 0.28849926342101484\n",
      "Epoch 5, Train Loss: 0.2401693253231447\n"
     ]
    }
   ],
   "source": [
    "cosine_ft(cosine_ft_lstm, device, ft_dataloader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_ft_context_preds = []\n",
    "\n",
    "for pair in test_data['x']:\n",
    "    s1, s2 = vocab.sequencify(pair[0]), vocab.sequencify(pair[1])\n",
    "\n",
    "    emb1 = get_context_embedding(cosine_ft_lstm, device, s1)\n",
    "    emb2 = get_context_embedding(cosine_ft_lstm, device, s2)\n",
    "\n",
    "    score = cosine(emb1, emb2) * 5\n",
    "    cosine_ft_context_preds.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Score for LSTM Model (from hidden state) with Cosine Finetuning: 0.7675\n"
     ]
    }
   ],
   "source": [
    "pearson_score, _ = pearsonr(cosine_ft_context_preds, test_data['y'])\n",
    "print(f'Pearson Score for LSTM Model (from hidden state) with Cosine Finetuning: {pearson_score:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3ebefefc6aaf644093121d8e5202d868a7e59c8d807fbf6949f2a61cd748adb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
