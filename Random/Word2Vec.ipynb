{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from src.Vocab import Vocab\n",
    "from src.utils import cosine\n",
    "from src.Word2Vec import Word2Vec\n",
    "import numpy as np\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/base_text_data.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(sentences, remove_stopwords=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = [vocab.sequencify(sen, addEOSBOS=True) for sen in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sequences, context_size, skipgram=False):\n",
    "        self.skipgram = skipgram\n",
    "        self.context_size = context_size\n",
    "\n",
    "        self.contexts = []\n",
    "        self.targets = []\n",
    "\n",
    "        for seq in sequences:\n",
    "            for i in range(self.context_size, len(seq) - self.context_size):\n",
    "                target = seq[i]\n",
    "                context = []\n",
    "                for j in range(i - self.context_size, i+self.context_size + 1):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    context.append(seq[j])\n",
    "                self.targets.append(target)\n",
    "                self.contexts.append(context)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = (self.targets[idx], np.array(self.contexts[idx]))\n",
    "        if self.skipgram:\n",
    "            return data\n",
    "        else:\n",
    "            return data[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dataset = EmbeddingDataset(train_sequences, CONTEXT_SIZE)\n",
    "embedding_dataloader = DataLoader(\n",
    "    embedding_dataset, shuffle=True, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (embedding): Embedding(12935, 128)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=12935, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2Vec(len(vocab), CONTEXT_SIZE, embedding_dim=EMBEDDING_DIM)\n",
    "w2v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:Word2Vec, dataloader:DataLoader, device:torch.device, n_epochs:int = 5) -> None:\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    model.train()\n",
    "\n",
    "    assert model.skipgram == dataloader.dataset.skipgram, \"Mismatching Model and Data Formats\"\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        loss_val = 0\n",
    "\n",
    "        for X, y in dataloader:\n",
    "            X = X.long().to(device)\n",
    "            y = y.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_val += loss.item()\n",
    "\n",
    "        print(f\"Epoch {e+1}, Train Loss: {loss_val/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 5.396146960933049\n",
      "Epoch 2, Train Loss: 4.571523395593045\n",
      "Epoch 3, Train Loss: 4.247083069079161\n",
      "Epoch 4, Train Loss: 4.071893863003414\n",
      "Epoch 5, Train Loss: 3.959844770467074\n",
      "Epoch 6, Train Loss: 3.882691705483687\n",
      "Epoch 7, Train Loss: 3.8219380388846553\n",
      "Epoch 8, Train Loss: 3.778209960203874\n",
      "Epoch 9, Train Loss: 3.7432921994739856\n",
      "Epoch 10, Train Loss: 3.713076945465265\n",
      "Epoch 11, Train Loss: 3.6900313506466156\n",
      "Epoch 12, Train Loss: 3.6698004051783704\n",
      "Epoch 13, Train Loss: 3.6548119341181446\n",
      "Epoch 14, Train Loss: 3.6416723964475066\n",
      "Epoch 15, Train Loss: 3.6345605269481847\n"
     ]
    }
   ],
   "source": [
    "train(w2v, embedding_dataloader, device, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(model:Word2Vec, sentence, device):\n",
    "    seq = vocab.sequencify(sentence)\n",
    "\n",
    "    embeddings = np.zeros(model.embedding.embedding_dim)\n",
    "\n",
    "    for idx in seq:\n",
    "        idx = torch.tensor(idx).long().to(device)\n",
    "        emb = model.embedding(idx).cpu().detach().numpy()\n",
    "        embeddings += emb\n",
    "    \n",
    "    return embeddings / len(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for pairs in test_data['x']:\n",
    "    s1 = pairs[0]\n",
    "    s2 = pairs[1]\n",
    "    v1 = get_sentence_embedding(w2v, s1, device)\n",
    "    v2 = get_sentence_embedding(w2v, s2, device)\n",
    "\n",
    "    score = cosine(v1, v2) * 5\n",
    "    preds.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Score for Word2Vec Model with Cosine Similarity: 0.6527\n"
     ]
    }
   ],
   "source": [
    "pearson_score, _ = pearsonr(preds, test_data['y'])\n",
    "print(f'Pearson Score for Word2Vec Model with Cosine Similarity: {pearson_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_words(model:Word2Vec, word, device):\n",
    "    model.eval()\n",
    "\n",
    "    idx = torch.tensor(vocab.get_idx(word)).long().to(device)\n",
    "    src = model.embedding(idx).cpu().detach().numpy()\n",
    "    word_pairs = []\n",
    "\n",
    "    for word in vocab:\n",
    "        idx = torch.tensor(vocab.get_idx(word)).long().to(device)\n",
    "        trg = model.embedding(idx).cpu().detach().numpy()\n",
    "        sim = cosine(trg, src)\n",
    "        word_pairs.append((word, sim))\n",
    "\n",
    "    word_pairs = sorted(word_pairs, key=lambda x: -x[1])\n",
    "    return word_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 1.0000000596046448),\n",
       " ('girl', 0.8020584285259247),\n",
       " ('man', 0.7542853951454163),\n",
       " ('child', 0.7368017733097076),\n",
       " ('person', 0.7338158488273621),\n",
       " ('woman', 0.7258056551218033),\n",
       " ('snowboarder', 0.7229256331920624),\n",
       " ('player', 0.7159172147512436),\n",
       " ('baby', 0.7051761299371719),\n",
       " ('kid', 0.7037726491689682)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words(w2v, 'boy', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSDataset(Dataset):\n",
    "    def __init__(self, vocab:Vocab, data):\n",
    "        self.sts_data = []\n",
    "        for x, y in zip(data['x'], data['y']):\n",
    "            s1, s2 = x\n",
    "            s1, s2 = vocab.sequencify(s1, addEOSBOS=True), vocab.sequencify(s2, addEOSBOS=True)\n",
    "            self.sts_data.append(((s1, s2), y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sts_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sts_data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dataset = STSDataset(vocab, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_ft_w2v = deepcopy(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_ft(model:Word2Vec, device:torch.device, dataloader:DataLoader, n_epochs:int=5):\n",
    "    criterion = nn.CosineSimilarity(dim=0)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        loss_val = 0\n",
    "\n",
    "        for X, y in dataloader:\n",
    "            s1, s2 = X[0]\n",
    "            s1 = torch.tensor(s1).long().to(device)\n",
    "            s2 = torch.tensor(s2).long().to(device)\n",
    "            y = torch.tensor(y).float().to(device).squeeze(0)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            emb1 = torch.mean(model.embedding(s1), dim=0)\n",
    "            emb2 = torch.mean(model.embedding(s2), dim=0)\n",
    "\n",
    "            sim = criterion(emb1, emb2) * 5\n",
    "            loss = nn.MSELoss()(sim, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_val += loss.item()\n",
    "        print(f\"Epoch {e+1}, Train Loss: {loss_val/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dataloader = DataLoader(ft_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: zip(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6823128048377483\n",
      "Epoch 2, Train Loss: 0.45681999826893227\n",
      "Epoch 3, Train Loss: 0.3302942856766977\n",
      "Epoch 4, Train Loss: 0.25560188880442875\n",
      "Epoch 5, Train Loss: 0.20488578198731108\n",
      "Epoch 6, Train Loss: 0.1688497619827834\n",
      "Epoch 7, Train Loss: 0.14156667873783454\n",
      "Epoch 8, Train Loss: 0.12002876869153267\n",
      "Epoch 9, Train Loss: 0.10307753202717057\n",
      "Epoch 10, Train Loss: 0.08999764502303989\n",
      "Epoch 11, Train Loss: 0.0781947272629363\n",
      "Epoch 12, Train Loss: 0.06957320787498535\n",
      "Epoch 13, Train Loss: 0.06215618388200053\n",
      "Epoch 14, Train Loss: 0.05620273006193558\n",
      "Epoch 15, Train Loss: 0.05107094679864024\n"
     ]
    }
   ],
   "source": [
    "cosine_ft(cosine_ft_w2v, device, ft_dataloader, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_ft_preds = []\n",
    "\n",
    "for pairs in test_data['x']:\n",
    "    s1 = pairs[0]\n",
    "    s2 = pairs[1]\n",
    "    v1 = get_sentence_embedding(cosine_ft_w2v, s1, device)\n",
    "    v2 = get_sentence_embedding(cosine_ft_w2v, s2, device)\n",
    "\n",
    "    score = cosine(v1, v2) * 5\n",
    "    cosine_ft_preds.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Score for Word2Vec Model with Cosine Finetuning: 0.8554\n"
     ]
    }
   ],
   "source": [
    "pearson_score, _ = pearsonr(cosine_ft_preds, test_data['y'])\n",
    "print(f'Pearson Score for Word2Vec Model with Cosine Finetuning: {pearson_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_ft_w2v = deepcopy(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoringHead(nn.Module):\n",
    "\n",
    "    def __init__(self, w2v:Word2Vec, input_dim:int):\n",
    "        super(ScoringHead, self).__init__()\n",
    "        self.w2v = w2v\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim_1 = 2048\n",
    "        self.hidden_dim_2 = 1024\n",
    "        self.hidden_dim_3 = 512\n",
    "        self.hidden_dim_4 = 256\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim_1, self.hidden_dim_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim_2, self.hidden_dim_3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim_3, self.hidden_dim_4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim_4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, s1:torch.tensor, s2:torch.tensor):\n",
    "        \n",
    "        emb1 = torch.mean(self.w2v.embedding(s1), dim=0)\n",
    "        emb2 = torch.mean(self.w2v.embedding(s2), dim=0)\n",
    "\n",
    "        # emb = torch.cat((emb1, emb2), dim=-1)\n",
    "        diff = torch.abs(emb1 - emb2)\n",
    "        out = self.linear_stack(diff)\n",
    "        return out * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScoringHead(\n",
       "  (w2v): Word2Vec(\n",
       "    (embedding): Embedding(12935, 128)\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=12935, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (linear_stack): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_head = ScoringHead(mlp_ft_w2v, EMBEDDING_DIM)\n",
    "scoring_head.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_ft(model:ScoringHead, device:torch.device, dataloader:DataLoader, n_epochs=5):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        loss_val = 0\n",
    "        for X, y in dataloader:\n",
    "            s1, s2 = X[0]\n",
    "            s1 = torch.tensor(s1).long().to(device)\n",
    "            s2 = torch.tensor(s2).long().to(device)\n",
    "            y = torch.tensor(y).float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(s1, s2)\n",
    "\n",
    "            loss = loss_fn(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_val += loss.item()\n",
    "\n",
    "        print(f\"Epoch {e+1}, Train Loss: {loss_val/len(dataloader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.8144275890690776\n",
      "Epoch 2, Train Loss: 0.49398384255347133\n",
      "Epoch 3, Train Loss: 0.2955798331155108\n",
      "Epoch 4, Train Loss: 0.19108123085568932\n",
      "Epoch 5, Train Loss: 0.1362320179472898\n",
      "Epoch 6, Train Loss: 0.11129868440946206\n",
      "Epoch 7, Train Loss: 0.09715406092106922\n",
      "Epoch 8, Train Loss: 0.08442426689808673\n",
      "Epoch 9, Train Loss: 0.0777957329676715\n",
      "Epoch 10, Train Loss: 0.07426489540505626\n",
      "Epoch 11, Train Loss: 0.06806227564148051\n",
      "Epoch 12, Train Loss: 0.0630297456150563\n",
      "Epoch 13, Train Loss: 0.06190243160014222\n",
      "Epoch 14, Train Loss: 0.05616168147227822\n",
      "Epoch 15, Train Loss: 0.055930369491746644\n"
     ]
    }
   ],
   "source": [
    "mlp_ft(scoring_head, device, ft_dataloader, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_score_predict(model, s1, s2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(s1, s2)\n",
    "\n",
    "    return pred.cpu().detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_ft_preds = []\n",
    "\n",
    "for pairs in test_data['x']:\n",
    "    s1 = pairs[0]\n",
    "    s2 = pairs[1]\n",
    "    s1, s2 = vocab.sequencify(s1, addEOSBOS=True), vocab.sequencify(s2, addEOSBOS=True)\n",
    "    s1 = torch.tensor(s1).to(device)\n",
    "    s2 = torch.tensor(s2).to(device)\n",
    "\n",
    "    score = mlp_score_predict(scoring_head, s1, s2)\n",
    "\n",
    "    mlp_ft_preds.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Score for Word2Vec with MLP Scoring: 0.7953\n"
     ]
    }
   ],
   "source": [
    "pearson_score, _ = pearsonr(mlp_ft_preds, test_data['y'])\n",
    "print(f'Pearson Score for Word2Vec with MLP Scoring: {pearson_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(w2v, 'models/w2v.pt')\n",
    "# torch.save(cosine_ft_w2v, 'models/cosine_ft_w2v.pt')\n",
    "# torch.save(scoring_head, 'models/mlp_ft_scoringhead_w2v.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3ebefefc6aaf644093121d8e5202d868a7e59c8d807fbf6949f2a61cd748adb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
