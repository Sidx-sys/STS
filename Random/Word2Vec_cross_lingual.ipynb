{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/sidharth.giri/miniconda3/envs/pytorch_dep/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from src.Vocab import Vocab\n",
    "from src.Vocab_es import Vocab_es\n",
    "from src.utils import cosine\n",
    "from src.Word2Vec import Word2Vec\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/train_data_cross.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/test_data_cross.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/base_text_data.pkl', 'rb') as f:\n",
    "    en_sentences = pickle.load(f)\n",
    "with open('../data/spanish_base_text_data.pkl', 'rb') as f:\n",
    "    es_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_en = Vocab(en_sentences, remove_stopwords=False)\n",
    "vocab_es = Vocab(es_sentences, remove_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_sequences = [vocab_en.sequencify(sen, addEOSBOS=True) for sen in en_sentences]\n",
    "es_train_sequences = [vocab_en.sequencify(sen, addEOSBOS=True) for sen in en_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sequences, context_size, skipgram=False):\n",
    "        self.skipgram = skipgram\n",
    "        self.context_size = context_size\n",
    "\n",
    "        self.contexts = []\n",
    "        self.targets = []\n",
    "\n",
    "        for seq in sequences:\n",
    "            for i in range(self.context_size, len(seq) - self.context_size):\n",
    "                target = seq[i]\n",
    "                context = []\n",
    "                for j in range(i - self.context_size, i+self.context_size + 1):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    context.append(seq[j])\n",
    "                self.targets.append(target)\n",
    "                self.contexts.append(context)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = (self.targets[idx], np.array(self.contexts[idx]))\n",
    "        if self.skipgram:\n",
    "            return data\n",
    "        else:\n",
    "            return data[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embedding_dataset = EmbeddingDataset(en_train_sequences, CONTEXT_SIZE)\n",
    "en_embedding_dataloader = DataLoader(\n",
    "    en_embedding_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "es_embedding_dataset = EmbeddingDataset(es_train_sequences, CONTEXT_SIZE)\n",
    "es_embedding_dataloader = DataLoader(\n",
    "    es_embedding_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (embedding): Embedding(12935, 128)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=12935, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_en = Word2Vec(len(vocab_en), CONTEXT_SIZE, embedding_dim=EMBEDDING_DIM)\n",
    "w2v_en.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:Word2Vec, dataloader:DataLoader, device:torch.device, n_epochs:int = 5) -> None:\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    model.train()\n",
    "\n",
    "    assert model.skipgram == dataloader.dataset.skipgram, \"Mismatching Model and Data Formats\"\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        loss_val = 0\n",
    "\n",
    "        for X, y in dataloader:\n",
    "            X = X.long().to(device)\n",
    "            y = y.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_val += loss.item()\n",
    "\n",
    "        print(f\"Epoch {e+1}, Train Loss: {loss_val/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 5.406969225668374\n",
      "Epoch 2, Train Loss: 4.578141788583999\n",
      "Epoch 3, Train Loss: 4.2595872003723985\n",
      "Epoch 4, Train Loss: 4.087196238814253\n",
      "Epoch 5, Train Loss: 3.979096287079063\n",
      "Epoch 6, Train Loss: 3.9039239155525203\n",
      "Epoch 7, Train Loss: 3.8484516735894676\n",
      "Epoch 8, Train Loss: 3.8049890902433496\n",
      "Epoch 9, Train Loss: 3.7718125420013733\n",
      "Epoch 10, Train Loss: 3.7453961081512723\n",
      "Epoch 11, Train Loss: 3.721790452090392\n",
      "Epoch 12, Train Loss: 3.706480112652099\n",
      "Epoch 13, Train Loss: 3.693569019029431\n",
      "Epoch 14, Train Loss: 3.6820282528196966\n",
      "Epoch 15, Train Loss: 3.6725668499513215\n"
     ]
    }
   ],
   "source": [
    "train(w2v_en, en_embedding_dataloader, device, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (embedding): Embedding(32689, 128)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=32689, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_es = Word2Vec(len(vocab_es), CONTEXT_SIZE, embedding_dim=EMBEDDING_DIM)\n",
    "w2v_es.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 5.497272349184174\n",
      "Epoch 2, Train Loss: 4.633380799621418\n",
      "Epoch 3, Train Loss: 4.315388902910702\n",
      "Epoch 4, Train Loss: 4.144333707244315\n",
      "Epoch 5, Train Loss: 4.0398765696704935\n",
      "Epoch 6, Train Loss: 3.9663001262855215\n",
      "Epoch 7, Train Loss: 3.9121746915178846\n",
      "Epoch 8, Train Loss: 3.8720273638966662\n",
      "Epoch 9, Train Loss: 3.8412193288315204\n",
      "Epoch 10, Train Loss: 3.816815501589763\n",
      "Epoch 11, Train Loss: 3.797781426079923\n",
      "Epoch 12, Train Loss: 3.7787266285733536\n",
      "Epoch 13, Train Loss: 3.766381328991848\n",
      "Epoch 14, Train Loss: 3.7603669885573745\n",
      "Epoch 15, Train Loss: 3.7529972961053635\n"
     ]
    }
   ],
   "source": [
    "train(w2v_es, es_embedding_dataloader, device, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(model:Word2Vec, sentence, device, lang='en'):\n",
    "    seq = vocab_en.sequencify(sentence) if lang == 'en' else vocab_es.sequencify(sentence)\n",
    "\n",
    "    embeddings = np.zeros(model.embedding.embedding_dim)\n",
    "\n",
    "    for idx in seq:\n",
    "        idx = torch.tensor(idx).long().to(device)\n",
    "        emb = model.embedding(idx).cpu().detach().numpy()\n",
    "        embeddings += emb\n",
    "    \n",
    "    return embeddings / len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for pairs in test_data['x']:\n",
    "    s1 = pairs[0]\n",
    "    s2 = pairs[1]\n",
    "    v1 = get_sentence_embedding(w2v_en, s1, device)\n",
    "    v2 = get_sentence_embedding(w2v_es, s2, device, lang='es')\n",
    "\n",
    "    score = cosine(v1, v2) * 5\n",
    "    preds.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Score for Word2Vec Model with Cosine Similarity: 0.0556\n"
     ]
    }
   ],
   "source": [
    "pearson_score, _ = pearsonr(preds, test_data['y'])\n",
    "print(f'Pearson Score for Word2Vec Model with Cosine Similarity: {pearson_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSDataset(Dataset):\n",
    "    def __init__(self, vocab_en:Vocab, vocab_es:Vocab, data):\n",
    "        self.sts_data = []\n",
    "        for x, y in zip(data['x'], data['y']):\n",
    "            s1, s2 = x\n",
    "            s1, s2 = vocab_en.sequencify(s1, addEOSBOS=True), vocab_es.sequencify(s2, addEOSBOS=True)\n",
    "            self.sts_data.append(((s1, s2), y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sts_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sts_data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dataset = STSDataset(vocab_en, vocab_es, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_ft_w2v_en = deepcopy(w2v_en)\n",
    "cosine_ft_w2v_es = deepcopy(w2v_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_ft(model_en:Word2Vec, model_es:Word2Vec, device:torch.device, dataloader:DataLoader, n_epochs:int=5):\n",
    "    criterion = nn.CosineSimilarity(dim=0)\n",
    "    optimizer = optim.Adam(list(model_en.parameters()) + list(model_es.parameters()))\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        loss_val = 0\n",
    "\n",
    "        for X, y in dataloader:\n",
    "            s1, s2 = X[0]\n",
    "            s1 = torch.tensor(s1).long().to(device)\n",
    "            s2 = torch.tensor(s2).long().to(device)\n",
    "            y = torch.tensor(y).float().to(device).squeeze(0)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            emb1 = torch.mean(model_en.embedding(s1), dim=0)\n",
    "            emb2 = torch.mean(model_es.embedding(s2), dim=0)\n",
    "\n",
    "            sim = criterion(emb1, emb2) * 5\n",
    "            loss = nn.MSELoss()(sim, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_val += loss.item()\n",
    "        print(f\"Epoch {e+1}, Train Loss: {loss_val/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dataloader = DataLoader(ft_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: zip(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 10.470523593199633\n",
      "Epoch 2, Train Loss: 4.038824695902157\n",
      "Epoch 3, Train Loss: 2.3871585206719725\n",
      "Epoch 4, Train Loss: 1.7057296704688507\n",
      "Epoch 5, Train Loss: 1.3281767872885943\n",
      "Epoch 6, Train Loss: 1.0680211524986984\n",
      "Epoch 7, Train Loss: 0.8596332850730188\n",
      "Epoch 8, Train Loss: 0.6861751152271408\n",
      "Epoch 9, Train Loss: 0.5442517368970222\n",
      "Epoch 10, Train Loss: 0.4306720170169178\n",
      "Epoch 11, Train Loss: 0.34014435868559373\n",
      "Epoch 12, Train Loss: 0.26942635434259504\n",
      "Epoch 13, Train Loss: 0.2157809375968393\n",
      "Epoch 14, Train Loss: 0.17400872588769292\n",
      "Epoch 15, Train Loss: 0.14150360114059593\n"
     ]
    }
   ],
   "source": [
    "cosine_ft(cosine_ft_w2v_en, cosine_ft_w2v_es, device, ft_dataloader, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_ft_preds = []\n",
    "\n",
    "for pairs in test_data['x']:\n",
    "    s1 = pairs[0]\n",
    "    s2 = pairs[1]\n",
    "    v1 = get_sentence_embedding(cosine_ft_w2v_en, s1, device)\n",
    "    v2 = get_sentence_embedding(cosine_ft_w2v_es, s2, device)\n",
    "\n",
    "    score = cosine(v1, v2) * 5\n",
    "    cosine_ft_preds.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Score for Word2Vec Model with Cosine Finetuning: 0.0264\n"
     ]
    }
   ],
   "source": [
    "pearson_score, _ = pearsonr(cosine_ft_preds, test_data['y'])\n",
    "print(f'Pearson Score for Word2Vec Model with Cosine Finetuning: {pearson_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_ft_w2v_en = deepcopy(w2v_en)\n",
    "mlp_ft_w2v_es = deepcopy(w2v_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoringHead(nn.Module):\n",
    "\n",
    "    def __init__(self, w2v_en:Word2Vec, w2v_es:Word2Vec, input_dim:int):\n",
    "        super(ScoringHead, self).__init__()\n",
    "        self.w2v_en = w2v_en\n",
    "        self.w2v_es = w2v_es\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim_1 = 2048\n",
    "        self.hidden_dim_2 = 1024\n",
    "        self.hidden_dim_3 = 512\n",
    "        self.hidden_dim_4 = 256\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim_1, self.hidden_dim_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim_2, self.hidden_dim_3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim_3, self.hidden_dim_4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim_4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, s1:torch.tensor, s2:torch.tensor):\n",
    "        \n",
    "        emb1 = torch.mean(self.w2v_en.embedding(s1), dim=0)\n",
    "        emb2 = torch.mean(self.w2v_es.embedding(s2), dim=0)\n",
    "\n",
    "        emb = torch.cat((emb1, emb2), dim=-1)\n",
    "        # diff = torch.abs(emb1 - emb2)\n",
    "        out = self.linear_stack(emb)\n",
    "        return out * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScoringHead(\n",
       "  (w2v_en): Word2Vec(\n",
       "    (embedding): Embedding(12935, 128)\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=12935, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (w2v_es): Word2Vec(\n",
       "    (embedding): Embedding(32689, 128)\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=32689, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (linear_stack): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_head = ScoringHead(mlp_ft_w2v_en, mlp_ft_w2v_es, 2 * EMBEDDING_DIM)\n",
    "scoring_head.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_ft(model:ScoringHead, device:torch.device, dataloader:DataLoader, n_epochs=5):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        loss_val = 0\n",
    "        for X, y in dataloader:\n",
    "            s1, s2 = X[0]\n",
    "            s1 = torch.tensor(s1).long().to(device)\n",
    "            s2 = torch.tensor(s2).long().to(device)\n",
    "            y = torch.tensor(y).float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(s1, s2)\n",
    "\n",
    "            loss = loss_fn(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_val += loss.item()\n",
    "\n",
    "        print(f\"Epoch {e+1}, Train Loss: {loss_val/len(dataloader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.8015419692480092\n",
      "Epoch 2, Train Loss: 1.3434802942550084\n",
      "Epoch 3, Train Loss: 0.7439725593557464\n",
      "Epoch 4, Train Loss: 0.37852614745298\n",
      "Epoch 5, Train Loss: 0.3172271911095775\n",
      "Epoch 6, Train Loss: 0.23393089628287006\n",
      "Epoch 7, Train Loss: 0.18910121809238284\n",
      "Epoch 8, Train Loss: 0.19297547649383928\n",
      "Epoch 9, Train Loss: 0.16098641081626627\n",
      "Epoch 10, Train Loss: 0.11604744388777243\n",
      "Epoch 11, Train Loss: 0.10586673323077365\n",
      "Epoch 12, Train Loss: 0.11498946858394639\n",
      "Epoch 13, Train Loss: 0.08536283282378697\n",
      "Epoch 14, Train Loss: 0.06902282667681682\n",
      "Epoch 15, Train Loss: 0.06107542514372912\n"
     ]
    }
   ],
   "source": [
    "mlp_ft(scoring_head, device, ft_dataloader, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_score_predict(model, s1, s2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(s1, s2)\n",
    "\n",
    "    return pred.cpu().detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_ft_preds = []\n",
    "\n",
    "for pairs in test_data['x']:\n",
    "    s1 = pairs[0]\n",
    "    s2 = pairs[1]\n",
    "    s1, s2 = vocab_en.sequencify(s1, addEOSBOS=True), vocab_es.sequencify(s2, addEOSBOS=True)\n",
    "    s1 = torch.tensor(s1).to(device)\n",
    "    s2 = torch.tensor(s2).to(device)\n",
    "\n",
    "    score = mlp_score_predict(scoring_head, s1, s2)\n",
    "\n",
    "    mlp_ft_preds.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Score for Word2Vec with MLP Scoring: 0.0477\n"
     ]
    }
   ],
   "source": [
    "pearson_score, _ = pearsonr(mlp_ft_preds, test_data['y'])\n",
    "print(f'Pearson Score for Word2Vec with MLP Scoring: {pearson_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Score for Word2Vec with MLP Scoring (Concat): 0.0477\n"
     ]
    }
   ],
   "source": [
    "pearson_score, _ = pearsonr(mlp_ft_preds, test_data['y'])\n",
    "print(f'Pearson Score for Word2Vec with MLP Scoring (Concat): {pearson_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(w2v, 'models/w2v.pt')\n",
    "# torch.save(cosine_ft_w2v, 'models/cosine_ft_w2v.pt')\n",
    "# torch.save(scoring_head, 'models/mlp_ft_scoringhead_w2v.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3ebefefc6aaf644093121d8e5202d868a7e59c8d807fbf6949f2a61cd748adb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
