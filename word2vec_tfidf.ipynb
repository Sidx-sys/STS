{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def file_to_data(path):\n",
    "#     try:\n",
    "#         with open(path) as f:\n",
    "#             data = [[x.rstrip().split('\\t')[1], x.rstrip().split('\\t')[2], x.rstrip().split('\\t')[0]]  for x in f.readlines()]\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"File does not exist\")\n",
    "#         return\n",
    "    \n",
    "#     formatted_data = []\n",
    "#     for row in data:\n",
    "#         formatted_data.append([row[0], row[1], row[2]])\n",
    "\n",
    "#     return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = '2016'\n",
    "# data_file = 'postediting.test.tsv'\n",
    "# path = f'data/sts/semeval-sts/{year}/{data_file}'\n",
    "# data = file_to_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two young, White males are outside near many bushes.',\n",
       " 'Several men in hard hats are operating a giant pulley system.',\n",
       " 'A little girl climbing into a wooden playhouse.',\n",
       " 'A man in a blue shirt is standing on a ladder cleaning a window.',\n",
       " 'Two men are at the stove preparing food.']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('data/Multi-30k/train.en') as f:\n",
    "#     data = [x.strip() for x in f.readlines()]\n",
    "# data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dat in data[15000:20000]:\n",
    "#     text_data.append(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60265"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_data = list(set(text_data))\n",
    "# len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/base_text_data.pkl', 'rb') as f:\n",
    "    text_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer_w2v:\n",
    "    def __init__(self):\n",
    "        self.punctuations = [r'\\.', r'\\.{2,}',\n",
    "                             r'\\!+', r'\\:+', r'\\;+', r'\\\"+', r\"\\'+\", r'\\?+', r'\\,+', r'\\(|\\)|\\[|\\]|\\{|\\}|\\<|\\>']\n",
    "\n",
    "    def clean(self, line):\n",
    "        for pattern in self.punctuations:\n",
    "            line = re.sub(pattern, '', line)\n",
    "        line = re.sub(r'[^a-z]', ' ', line.lower())\n",
    "        return line\n",
    "\n",
    "    def tokenize(self, line):\n",
    "        line = self.clean(line)\n",
    "        return line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review:\n",
    "    tokenizer = Tokenizer_w2v()\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.tokens = Review.tokenizer.tokenize(self.text)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tokens)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "for r in text_data:\n",
    "    training_data.append(Review(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        input_embeds = self.embeddings(inputs)\n",
    "        embeds = torch.mean(input_embeds, dim=1)\n",
    "        out = self.linear(embeds)\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, reviews, context_size=2, embedding_size=50, oov_threshold=2, neg_sample_size=5, lr=0.001):\n",
    "\n",
    "        self.reviews = reviews\n",
    "        self.oov_threshold = oov_threshold\n",
    "        self.oov_token = '<OOV>'\n",
    "        self.context_size = context_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.vocabulary = {self.oov_token}\n",
    "        self.vocab_idx = {self.oov_token: 0}\n",
    "        self.vocab_ridx = {0: self.oov_token}\n",
    "\n",
    "        self.freq = defaultdict(int)\n",
    "        self.freq_dist = [0]\n",
    "        self.total_word_count = 0\n",
    "\n",
    "        self.build_vocabulary()\n",
    "\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.neg_sample_size = neg_sample_size\n",
    "\n",
    "        self.model = CBOW(self.N, self.embedding_size)\n",
    "        self.dataset = self.build_dataset()\n",
    "        self.weights = self.negative_sampling()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr)\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "        \n",
    "        print(\"Building Vocabulary\")\n",
    "        for review in tqdm(self.reviews):\n",
    "            for token in review:\n",
    "                self.freq[token] += 1\n",
    "\n",
    "        index = 1\n",
    "        for token, f in self.freq.items():\n",
    "            if f > self.oov_threshold:\n",
    "                self.vocabulary.add(token)\n",
    "                self.vocab_idx[token] = index\n",
    "                self.vocab_ridx[index] = token\n",
    "                self.freq_dist.append(f)\n",
    "                index += 1\n",
    "            else:\n",
    "                self.freq_dist[0] += f\n",
    "\n",
    "        self.total_word_count = sum(self.freq.values())        \n",
    "        self.N = len(self.vocabulary)\n",
    "        print(f\"Total Vocabulary Size: {self.N}\")\n",
    "\n",
    "    def build_dataset(self):\n",
    "\n",
    "        print(\"Building Dataset\")\n",
    "        dataset = []\n",
    "        for review in tqdm(self.reviews):\n",
    "            for i in range(self.context_size, len(review) - self.context_size):\n",
    "                focus = review[i]\n",
    "                if focus not in self.vocabulary:\n",
    "                    focus = self.oov_token\n",
    "                focus_index = self.vocab_idx[focus]\n",
    "                context_indices = []\n",
    "                for j in range(i - self.context_size, i + self.context_size + 1):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    context = review[j]\n",
    "                    if context not in self.vocabulary:\n",
    "                        context = self.oov_token\n",
    "                    context_index = self.vocab_idx[context]\n",
    "                    context_indices.append(context_index)\n",
    "                dataset.append((context_indices, focus_index))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def negative_sampling(self):\n",
    "        print(\"Computing Weights\")\n",
    "        normalized_freq = F.normalize(torch.Tensor(self.freq_dist).pow(0.75), dim=0)\n",
    "        weights = torch.ones(len(self.freq_dist))\n",
    "\n",
    "        for _ in tqdm(range(len(self.freq_dist))):\n",
    "            for _ in range(self.neg_sample_size):\n",
    "                neg_index = torch.multinomial(normalized_freq, 1)[0]\n",
    "                weights[neg_index] += 1\n",
    "        \n",
    "        return weights\n",
    "\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        losses = []\n",
    "        loss_fn = nn.NLLLoss(weight=self.weights)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            net_loss = 0\n",
    "            for i in tqdm(range(0, len(self.dataset), self.BATCH_SIZE)):\n",
    "                batch = self.dataset[i: i+self.BATCH_SIZE]\n",
    "\n",
    "                context = [x[0] for x in batch]\n",
    "                focus = [x[1] for x in batch]\n",
    "\n",
    "                context_var = Variable(torch.LongTensor(context))\n",
    "                focus_var = Variable(torch.LongTensor(focus))\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                log_probs = self.model(context_var)\n",
    "                loss = loss_fn(log_probs, focus_var)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                net_loss += loss.item()\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            losses.append(net_loss)\n",
    "\n",
    "    \n",
    "    def get_embedding(self, word_idx):\n",
    "        embedding_index = Variable(torch.LongTensor([word_idx]))\n",
    "        return self.model.embeddings(embedding_index).data[0]\n",
    "    \n",
    "    def get_closest_vector(self, _word, k):\n",
    "        \n",
    "        word = _word.lower()\n",
    "\n",
    "        if word not in self.vocabulary:\n",
    "            word = self.oov_token\n",
    "\n",
    "        distances = []\n",
    "        focus_index = self.vocab_idx[word]\n",
    "        focus_embedding = self.get_embedding(focus_index)\n",
    "\n",
    "        for i in range(1, self.N):\n",
    "            if i == focus_index:\n",
    "                continue\n",
    "        \n",
    "            comp_embedding = self.get_embedding(i)\n",
    "            comp_word = self.vocab_ridx[i]\n",
    "            dist = cosine(focus_embedding, comp_embedding)\n",
    "            distances.append({'Word': comp_word, 'Distance': dist})\n",
    "        \n",
    "        distances = sorted(distances, key=lambda x: x['Distance'])\n",
    "\n",
    "        return [x['Word'] for x in distances[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2vec_models/word2vec_model_35.pkl', 'rb') as f:\n",
    "    word2vec_model_35 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer_TFIDF:\n",
    "    def __init__(self):\n",
    "        self.regex_subs = {\n",
    "            r'(https?:\\/\\/)\\S+': \"0URL0\",\n",
    "            r'(?<!http://)www\\.\\S+': \"0URL0\",\n",
    "            r'(\\W)(?=\\1)': '',\n",
    "            r'(?<=[a-zA-Z])(\\-)(?=[a-zA-Z])': ''\n",
    "        }\n",
    "\n",
    "        self.punctuations = [r'\\.', r'\\.{2,}',\n",
    "                             r'\\!+', r'\\:+', r'\\;+', r'\\\"+', r\"\\'+\", r'\\?+', r'\\,+', r'\\(|\\)|\\[|\\]|\\{|\\}|\\<|\\>']\n",
    "\n",
    "        self.delimiter = '<SPLIT>'\n",
    "        self.stemmer = SnowballStemmer(language='english')    \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def clean_line(self, line):\n",
    "        for pattern, rep in self.regex_subs.items():\n",
    "            line = re.sub(pattern, rep, line)\n",
    "        for pattern in self.punctuations:\n",
    "            line = re.sub(pattern, '', line)\n",
    "        return line.lower()\n",
    "\n",
    "    def tokenize_line(self, line):\n",
    "        line = re.sub('\\s+', self.delimiter, line)\n",
    "\n",
    "        token_list = [x.strip()\n",
    "                      for x in line.split(self.delimiter) if x.strip() != '']\n",
    "\n",
    "        return token_list\n",
    "\n",
    "    def clean_and_tokenize(self, lines):\n",
    "        if isinstance(lines, list):\n",
    "            cleaned_tokens = []\n",
    "            for line in lines:\n",
    "                if not len(line.strip()):\n",
    "                    continue\n",
    "                line = self.clean_line(line)\n",
    "                tokens = self.tokenize_line(line)\n",
    "                cleaned_tokens.append(tokens)\n",
    "            return cleaned_tokens\n",
    "        else:\n",
    "            line = self.clean_line(lines)\n",
    "            tokens = self.tokenize_line(line)\n",
    "            return tokens\n",
    "\n",
    "    def _clean(self, line):\n",
    "        line = self.clean_line(line)\n",
    "        \n",
    "        cleaned = []\n",
    "        for token in line.split():\n",
    "            if token not in self.stop_words:\n",
    "                cleaned.append(self.stemmer.stem(token))\n",
    "\n",
    "        return \" \".join(cleaned)\n",
    "\n",
    "    def clean(self, lines):\n",
    "        if isinstance(lines, list):\n",
    "            cleaned_lines = []\n",
    "            for line in lines:\n",
    "                if not len(line.strip()):\n",
    "                    continue\n",
    "                line = self._clean(line)\n",
    "                cleaned_lines.append(line)\n",
    "            return cleaned_lines\n",
    "        else:\n",
    "            line = self._clean(lines)\n",
    "            return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizer():\n",
    "    def __init__(self):\n",
    "        self.tf_l = []\n",
    "        self.tf_r = []\n",
    "\n",
    "        self.idf = defaultdict(int)\n",
    "\n",
    "        self.vocab = {}\n",
    "        self.data_l = None\n",
    "        self.data_r = None\n",
    "        self.vocab_len = 0\n",
    "        self.num_docs = 0\n",
    "\n",
    "    def create_vocab(self, data):\n",
    "        print(\"Creating Vocabulary...\")\n",
    "        self.data_l = []\n",
    "        self.data_r = []\n",
    "        for items in data:\n",
    "            self.data_l.append(items[0].split())\n",
    "            self.data_r.append(items[1].split())\n",
    "        self.num_docs = len(self.data_l)\n",
    "\n",
    "        for text in self.data_l:\n",
    "            for token in text:\n",
    "                if not token in self.vocab:\n",
    "                    self.vocab[token] = self.vocab_len\n",
    "                    self.vocab_len += 1\n",
    "        \n",
    "        for text in self.data_r:\n",
    "            for token in text:\n",
    "                if not token in self.vocab:\n",
    "                    self.vocab[token] = self.vocab_len\n",
    "                    self.vocab_len += 1\n",
    "\n",
    "    def compute_tf(self):\n",
    "        print(\"Computing TF Scores...\")\n",
    "        for text in self.data_l:\n",
    "            d = defaultdict(int)\n",
    "            for token in text:\n",
    "                d[self.vocab[token]] += 1\n",
    "            self.tf_l.append(d)\n",
    "        \n",
    "        for text in self.data_r:\n",
    "            d = defaultdict(int)\n",
    "            for token in text:\n",
    "                d[self.vocab[token]] += 1\n",
    "            self.tf_r.append(d)\n",
    "    \n",
    "    def compute_idf(self):\n",
    "        print(\"Computing IDF Scores...\")\n",
    "        for token in self.vocab:\n",
    "            df = 0\n",
    "            for text in self.data_l:\n",
    "                if token in text:\n",
    "                    df += 1\n",
    "            \n",
    "            for text in self.data_r:\n",
    "                if token in text:\n",
    "                    df += 1\n",
    "        \n",
    "            self.idf[self.vocab[token]] = math.log((1 + self.num_docs)/(1 + df)) + 1\n",
    "     \n",
    "    def fit_transform(self, data):\n",
    "        self.create_vocab(data)\n",
    "        self.compute_tf()\n",
    "        self.compute_idf()\n",
    "        print(\"Creating TF-IDF Vectors...\")\n",
    "        X_l = np.zeros((self.num_docs, self.vocab_len), dtype='float32')\n",
    "        X_r = np.zeros((self.num_docs, self.vocab_len), dtype='float32')\n",
    "\n",
    "        for i in range(self.num_docs):\n",
    "            for token in self.data_l[i]:\n",
    "                X_l[i][self.vocab[token]] = self.tf_l[i][self.vocab[token]] * self.idf[self.vocab[token]]\n",
    "        \n",
    "        for i in range(self.num_docs):\n",
    "            for token in self.data_r[i]:\n",
    "                X_r[i][self.vocab[token]] = self.tf_r[i][self.vocab[token]] * self.idf[self.vocab[token]]\n",
    "        \n",
    "        return X_l, X_r\n",
    "    \n",
    "    def transform(self, data):\n",
    "        data_l = []\n",
    "        data_r = []\n",
    "        tf_l = []\n",
    "        tf_r = []\n",
    "        num_docs = len(data)\n",
    "\n",
    "        for items in data:\n",
    "            data_l.append(items[0].split())\n",
    "            data_r.append(items[1].split())\n",
    "\n",
    "        for text in data_l:\n",
    "            d = defaultdict(int)\n",
    "            for token in text:\n",
    "                if token in self.vocab:\n",
    "                    d[self.vocab[token]] += 1\n",
    "            tf_l.append(d)\n",
    "\n",
    "        for text in data_r:\n",
    "            d = defaultdict(int)\n",
    "            for token in text:\n",
    "                if token in self.vocab:\n",
    "                    d[self.vocab[token]] += 1\n",
    "            tf_r.append(d)\n",
    "        \n",
    "        X_l = np.zeros((num_docs, self.vocab_len), dtype='float32')\n",
    "        X_r = np.zeros((num_docs, self.vocab_len), dtype='float32')\n",
    "\n",
    "        for i in range(num_docs):\n",
    "            for token in data_l[i]:\n",
    "                if token in self.vocab:\n",
    "                    X_l[i][self.vocab[token]] = tf_l[i][self.vocab[token]] * self.idf[self.vocab[token]]\n",
    "        \n",
    "        for i in range(num_docs):\n",
    "            for token in data_r[i]:\n",
    "                if token in self.vocab:\n",
    "                    X_r[i][self.vocab[token]] = tf_r[i][self.vocab[token]] * self.idf[self.vocab[token]]\n",
    "        \n",
    "        return X_l, X_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "tokenizer_tfidf = Tokenizer_TFIDF()\n",
    "tokenizer_w2v = Tokenizer_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_data['x']\n",
    "y_test = test_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_vector(word2vec_model, tfidf_model, tfidf_tokenizer, w2v_tokenizer, sent):\n",
    "    vec = np.zeros(word2vec_model.embedding_size)\n",
    "    tfidf_sent = tfidf_tokenizer.clean(sent)\n",
    "    tf_idf_vector = np.squeeze(tfidf_model.transform([[tfidf_sent, '']])[0])\n",
    "    tok_used = 0\n",
    "\n",
    "    for token in w2v_tokenizer.tokenize(sent):\n",
    "        tfidf_token = tfidf_tokenizer.clean(token)\n",
    "        if tfidf_token == '':\n",
    "            continue\n",
    "        try:\n",
    "            tfidf_token_idx = tfidf_model.vocab[tfidf_token]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        tf_idf_weight = tf_idf_vector[tfidf_token_idx]\n",
    "        try:\n",
    "            w2v_vec = word2vec_model.get_embedding(word2vec_model_35.vocab_idx[token]).detach().numpy()\n",
    "        except KeyError:\n",
    "            continue\n",
    "        vec += w2v_vec * tf_idf_weight\n",
    "        tok_used += 1\n",
    "    \n",
    "    return vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_1, vec_2):\n",
    "    return vec_1@vec_2.T/(np.linalg.norm(vec_1) * np.linalg.norm(vec_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for x in x_test:\n",
    "    vec_1 = get_weighted_vector(word2vec_model_35, vectorizer, tokenizer_tfidf, tokenizer_w2v, x[0])\n",
    "    vec_2 = get_weighted_vector(word2vec_model_35, vectorizer, tokenizer_tfidf, tokenizer_w2v, x[1])\n",
    "    preds.append(5 * cosine_similarity(vec_1, vec_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5782066792384322"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_score, _ = scipy.stats.pearsonr(preds, y_test)\n",
    "pearson_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec with weighted TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6248150712858092"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_score, _ = scipy.stats.pearsonr(preds, y_test)\n",
    "pearson_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec With TF-IDF Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6327463478758604"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_score, _ = scipy.stats.pearsonr(preds, y_test)\n",
    "pearson_score"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c8b4332f17b55abe5b67c7fb84c98e7a77ec1afd164452ea9e618fbfab6a3af"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
